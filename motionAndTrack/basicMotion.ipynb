{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motion detection**\n",
    "from https://pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/\n",
    "\n",
    "This article will detail how to build a basic motion detection and tracking system using computer vision techniques. This example will work with both pre-recorded videos and live streams from your webcam. This system will be developped or laptops/desktops.\n",
    "\n",
    "**Background subtraction intro**\n",
    "\n",
    "Background subtraction is critical in many computer vision applications. We use it to count the number of cars passing through a toll booth. We use it to count the number of people walking in and out of a store. *And we use it for motion detection.*\n",
    "\n",
    "Before we get started coding in this post, let me say that there are many, many ways to perform motion detection, tracking, and analysis in OpenCV:\n",
    "- cv2.BackgroundSubtractorMOG function. See http://www.ee.surrey.ac.uk/CVSSP/Publications/papers/KaewTraKulPong-AVBS01.pdf\n",
    "- cv2.BackgroundSubtractorMOG2 function. See ...\n",
    "\n",
    "Newer version of openCV have probability based foreground and background segmentation (see https://goldberg.berkeley.edu/pubs/acc-2012-visual-tracking-final.pdf) through the cv2.createBackgroundSubtractorGMG function in openCV 3.\n",
    "\n",
    "In motion detection, we tend to make the following assumption:\n",
    "\n",
    "*The background of our video stream is largely static and unchanging over consecutive frames of a video. Therefore, if we can model the background,if there is a substantial change, we can detect it — this change normally corresponds to **motion** on our video.*\n",
    "\n",
    "In the real-world this assumption can easily fail. Due to shadowing, reflections, lighting conditions, and any other possible change in the environment, our background can look quite different in various frames of a video. The most successful background subtraction/foreground detection systems utilize fixed mounted cameras and controlled lighting conditions.\n",
    "\n",
    "The methods explained above while effective, can also be computationally expensive. We'll keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import datetime\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-v\", \"--video\", help=\"path to the video file\")\n",
    "ap.add_argument(\"-a\", \"--min-area\", type=int, default=500, help=\"minimum area size\")\n",
    "args = vars(ap.parse_args())\n",
    "# if the video argument is None, then we are reading from webcam\n",
    "if args.get(\"video\", None) is None:\n",
    "\tvs = VideoStream(src=0).start()\n",
    "\ttime.sleep(2.0)\n",
    "# otherwise, we are reading from a video file\n",
    "else:\n",
    "\tvs = cv2.VideoCapture(args[\"video\"])\n",
    "# initialize the first frame in the video stream\n",
    "firstFrame = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **--video**, is optional. It simply defines a path to a pre-recorded video file that we can detect motion in. If you do not supply a path to a video file, then OpenCV will utilize your webcam to detect motion.\n",
    "- **--min-area**, which is the minimum size (in pixels) for a region of an image to be considered actual “motion”. We’ll often find small regions of an image that have changed substantially, likely due to noise or changes in lighting conditions so we’ll define a minimum size of a region to combat and filter out these false-positives.\n",
    "\n",
    "We’ll grab a reference to the webcam and wait for it to warm up. And if a video file is supplied, then we’ll create a pointer to it.\n",
    "\n",
    "*Assumption:* **The first frame of our video file will contain no motion and just background — therefore, we can model the background of our video stream using only the first frame of the video.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the frames of the video\n",
    "while True:\n",
    "\t# grab the current frame and initialize the occupied/unoccupied\n",
    "\t# text\n",
    "\tframe = vs.read()\n",
    "\tframe = frame if args.get(\"video\", None) is None else frame[1]\n",
    "\ttext = \"Unoccupied\"\n",
    "\t# if the frame could not be grabbed, then we have reached the end\n",
    "\t# of the video\n",
    "\tif frame is None:\n",
    "\t\tbreak\n",
    "\t# resize the frame, convert it to grayscale, and blur it\n",
    "\tframe = imutils.resize(frame, width=500)\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tgray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\t# if the first frame is None, initialize it\n",
    "\tif firstFrame is None:\n",
    "\t\tfirstFrame = gray\n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0596c81d6325fd795ff544f846f29d10b3e1dd0dc705e114bff3e8628efdd98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
